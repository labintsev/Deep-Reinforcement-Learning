# Deep-Reinforcement-Learning
Forked from Deep-Reinforcement-Learning-Hands-On-Second-Edition, published by Packt  
Практика для курса по обучению с подкреплением. 
В отличии от оригинальных примеров кода, вместо заброшенного OpenAI gym используется поддерживаемый Farama gymnasium.   
На занятиях мы будем постепенно разобирать примеры из книги. 
Весь код скорей всего не успеем, но для всерьез заинтересующихся будет создан фундамент для самостоятельного освоения RL. 

## Балльно-рейтинговая система 

Лабораторные работы по основным темам согласно РПД 4 * 5 = 20 баллов.   
Контрольная работа - 8 баллов.  
Тест на кампусе по теории (Маковейчук) — 4 балла.   
Практические задания перед каждой лабораторной работой  4 * 2 = 8.  
Итого 20+8+4+8 = 40.  
Дополнительно посещаемость лекций 0,5 * 8 = 4 балла.  
Текущие баллы [здесь](docs/scores.csv)  

## Начало работы 

КРАЙНЕ РЕКОМЕНДУЕТСЯ использовать ОС Ubuntu или Mac, т.к. библиотека gymnasium официально на Windows не поддерживается.   

Создайте и опубликуйте на github свой репозиторий со стандартной структурой. 

```
rl_course/
├─ sem1/
│  ├─ ...
│  ├─ 01_agent_anatomy.py
├─ sem2/
│  ├─ 01_cartpole.py
├─ .gitignore
├─ readme.md
├─ requirements.txt
```
В папки с содержанием семинаров `sem1, ...` добавляйте скрипты по мере прохождения курса.  
Сроки и объем выполненных заданий будут оцениваться по Вашим коммитам. 

## Семинар 1. Знакомство со средой

Задание:  
1. Опубликовать свой репозиторий и дать мне доступ, если репозиторий закрытый.
2. Модифицировать обертку в файле `03_random_action_wrapper.py` так, чтобы с заданной вероятностью действие было не случайным, а равно `1` (агент двигался вправо).

## Семинар 2. Cross Entropy Method

Задание:
1. Скопируйте файлы `docs/report_1.md` и `Chapter04/01_cartpole.py` в папку `sem2`. 
2. Исследуйте влияние размера скрытого слоя нейросети на скорость сходимости обучения (2 балла). 
Для каждого значения `hidden_state` нужно провести не менее 5 экспериментов. 
Записи логов тензорборды нужно добавить в гит, чтобы были видны результаы экспериментов.
3. Исследуйте влияние архитектуры нейросети на скорость сходимости обучения (3 балла). 
4. Сделайте запись видео с лучшим результатом (2 балла). Добавьте его в гит. 
5. Не забудьте сделать commit and push для всех файлов, включая логи, скриншоты графиков и видео!


## Семинар 3. Bellman equation introduction

Знакомство с уравнением Беллмана и методом итерации по ценностям состояния.  
Задание:  
1. Изучить среду `Frozen Lake`.  
2. Разобраться с кодом `Chapter05/01_frozenlake_v_iteration.py`.  


## Семинар 4. Q-learning for FrozenLake

Задание: 
1. Возьмите за основу шаблон отчета `docs/report_2.md`. 
2. Сравните алгоритмы V и Q learning (2 балла) 
3. Исследуйте влияние гиперпараметра `GAMMA` на скорость сходимости Q-learning. (2 балла) 
4. Сравнените алгоритмы V и Q learning на поле большего размера (3 балла) 

Каждый эксперимент проводить не менее 5 раз. 
В репозиторий закоммитить отчет, графики и содержимое папки `sem4/runs`  
Каждый пункт исследования должен содержать вывод, основанный на результатах экспериментов. 


## Семинар 5. Deep Q-learning
Аппроксимация функции ценности действия с помощью искуственной нейронной сети DQN(s,a). 
Для обучения в среде Pong рекомендуется использовать GPU. 

Задание: 
1. Изучите алгоритм табличного обучения (tabular Q learning) в среде Frozen Lake. `Chapter06/01_frozenlake_q_learning.py`. Реализуйте случайный выбор действия при нулевой ценности. Исследуйте влияние гиперпараметра альфа на среднее количество шагов обучения (>=5 повторений). (2 балла) 
2. Изучите алгоритм глубокого обучения (Deep Q learning) в среде Pong: `Chapter06/02_dqn_pong.py` Обучите сеть с гиперпараметрами по умолчанию и запишите видео `Chapter06/03_dqn_play.py` (2 балла). 
3. Исследуйте влияние гиперпараметра (на Ваш выбор) на среднее количество шагов обучения (>=2 повторений). (3 балла). 

В репозиторий закоммитить отчет, графики и содержимое папки `sem5/runs`  
Каждый пункт исследования должен содержать вывод, основанный на результатах экспериментов. 


## Семинар 6. REINFORCE and Actor-Critic methods
Комбинация политики и функции ценности действия. 
Для обучения в среде Pong рекомендуется использовать GPU. 

`Chapter11/02_cartpole_reinforce.py` 
`/Chapter11/02_pong_a2c.py`

## Семинар 7. Техники ускорения обучения с подкреплением. 
Конрольная работа. 


## Семинар 8. Многоагентные системы. 
Без оцениваемых заданий. 

